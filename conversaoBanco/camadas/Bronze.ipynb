{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c247a41-89ed-443b-aae3-1c98cda44cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo clientes.csv movido para a Bronze Zone.\nArquivo imovel.csv movido para a Bronze Zone.\nArquivo cobertura.csv movido para a Bronze Zone.\nArquivo apoliceseguro.csv movido para a Bronze Zone.\nArquivo corretor.csv movido para a Bronze Zone.\nArquivo seguradora.csv movido para a Bronze Zone.\n"
     ]
    }
   ],
   "source": [
    "# bronze-zone\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "from azure.core.exceptions import AzureError\n",
    "\n",
    "# Configuração do Azure Blob Storage\n",
    "azure_connection_string = \"-\"\n",
    "\n",
    "# Nome dos containers para cada camada\n",
    "landing_container_name = \"landing-zone\"\n",
    "bronze_container_name = \"bronze\"\n",
    "\n",
    "# Conectar ao Azure Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(azure_connection_string)\n",
    "\n",
    "# Função para copiar arquivo de um container para outro\n",
    "def copy_to_bronze(file_name):\n",
    "    try:\n",
    "        # Obter o cliente do blob na Landing Zone\n",
    "        source_blob = blob_service_client.get_blob_client(container=landing_container_name, blob=file_name)\n",
    "\n",
    "        # Obter o cliente do blob na Bronze Zone\n",
    "        target_blob = blob_service_client.get_blob_client(container=bronze_container_name, blob=file_name)\n",
    "\n",
    "        # Copiar o arquivo para a Bronze Zone\n",
    "        target_blob.start_copy_from_url(source_blob.url)\n",
    "        print(f\"Arquivo {file_name} movido para a Bronze Zone.\")\n",
    "    except AzureError as e:\n",
    "        print(f\"Ocorreu um erro ao mover o arquivo para a Bronze Zone: {e}\")\n",
    "\n",
    "\n",
    "# Lista de arquivos para processar\n",
    "files_to_process = [\n",
    "    \"clientes.csv\",\n",
    "    \"imovel.csv\",\n",
    "    \"cobertura.csv\",\n",
    "    \"apoliceseguro.csv\",\n",
    "    \"corretor.csv\",\n",
    "    \"seguradora.csv\"\n",
    "]\n",
    "\n",
    "# Processar cada arquivo da Landing para a Bronze Zone\n",
    "for file_name in files_to_process:\n",
    "    copy_to_bronze(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ded6214-b670-4786-b718-86ae164eba6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4296399392906826>, line 26\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m landing_table \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(input_path)\n",
       "\u001B[1;32m     24\u001B[0m landing_table \u001B[38;5;241m=\u001B[39m landing_table\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdt_insert_bronze\u001B[39m\u001B[38;5;124m\"\u001B[39m, current_timestamp())\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m\"\u001B[39m, lit(table))\n",
       "\u001B[0;32m---> 26\u001B[0m landing_table\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39msave(output_path)\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTabela \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m processada e salva na camada bronze.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [DELTA_PATH_EXISTS] Cannot write to already existent path wasbs://bronze@datalakea04fb344bd6a3620.blob.core.windows.net/cliente without setting OVERWRITE = 'true'."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[DELTA_PATH_EXISTS] Cannot write to already existent path wasbs://bronze@datalakea04fb344bd6a3620.blob.core.windows.net/cliente without setting OVERWRITE = 'true'."
       },
       "metadata": {
        "errorSummary": "[DELTA_PATH_EXISTS] Cannot write to already existent path wasbs://bronze@datalakea04fb344bd6a3620.blob.core.windows.net/cliente without setting OVERWRITE = 'true'. SQLSTATE: 42K04"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DELTA_PATH_EXISTS",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42K04",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-4296399392906826>, line 26\u001B[0m\n\u001B[1;32m     22\u001B[0m landing_table \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(input_path)\n\u001B[1;32m     24\u001B[0m landing_table \u001B[38;5;241m=\u001B[39m landing_table\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdt_insert_bronze\u001B[39m\u001B[38;5;124m\"\u001B[39m, current_timestamp())\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m\"\u001B[39m, lit(table))\n\u001B[0;32m---> 26\u001B[0m landing_table\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39msave(output_path)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTabela \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m processada e salva na camada bronze.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [DELTA_PATH_EXISTS] Cannot write to already existent path wasbs://bronze@datalakea04fb344bd6a3620.blob.core.windows.net/cliente without setting OVERWRITE = 'true'."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "storageAccountName = \"datalakea04fb344bd6a3620\"\n",
    "\n",
    "tables = [\"cliente\", \"imovel\", \"cobertura\", \"apolice_seguro\", \"corretor\", \"seguradora\"]\n",
    "\n",
    "sas_token = \"-\"\n",
    "\n",
    "for table in tables:\n",
    "    input_path = f\"wasbs://landing-zone@{storageAccountName}.blob.core.windows.net/{table}\"\n",
    "    output_path = f\"wasbs://bronze@{storageAccountName}.blob.core.windows.net/{table}\"\n",
    "    \n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.sas.landing-zone.{storageAccountName}.blob.core.windows.net\",\n",
    "        sas_token\n",
    "    )\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.sas.bronze.{storageAccountName}.blob.core.windows.net\",\n",
    "        sas_token\n",
    "    )\n",
    "    \n",
    "    landing_table = spark.read.option(\"header\", \"true\").csv(input_path)\n",
    "    \n",
    "    landing_table = landing_table.withColumn(\"dt_insert_bronze\", current_timestamp()).withColumn(\"filename\", lit(table))\n",
    "    \n",
    "    landing_table.write.format('delta').save(output_path)\n",
    "    \n",
    "    print(f\"Tabela {table} processada e salva na camada bronze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d943e152-1c4c-4f78-939e-a6821e237039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------+------------------+-------------+-----------+--------------------+---------+\n|cobertura_id|           descricao| premio|limite_indenizacao|seguradora_id|obrigatorio|    dt_insert_bronze| filename|\n+------------+--------------------+-------+------------------+-------------+-----------+--------------------+---------+\n|           1|Relationship cost...|1743.00|          43009.00|            1|      false|2024-12-02 00:04:...|cobertura|\n|           2|Everyone carry le...|8930.00|          58782.00|            2|      false|2024-12-02 00:04:...|cobertura|\n|           3|Congress cup sign...|2062.00|           4304.00|            3|       true|2024-12-02 00:04:...|cobertura|\n|           4|Worker hard baby ...|2086.00|          69797.00|            4|       true|2024-12-02 00:04:...|cobertura|\n|           5|Natural garden se...|5612.00|          53418.00|            5|       true|2024-12-02 00:04:...|cobertura|\n|           6|Major several dis...|9075.00|          21374.00|            6|      false|2024-12-02 00:04:...|cobertura|\n|           7|Land position sto...| 167.00|          11289.00|            7|      false|2024-12-02 00:04:...|cobertura|\n|           8|Say sure garden c...| 208.00|           9891.00|            8|       true|2024-12-02 00:04:...|cobertura|\n|           9|Executive woman p...| 981.00|          60181.00|            9|      false|2024-12-02 00:04:...|cobertura|\n|          10|Team production t...|7635.00|          93483.00|           10|       true|2024-12-02 00:04:...|cobertura|\n|          11|Pretty two room o...|7867.00|           7985.00|           11|      false|2024-12-02 00:04:...|cobertura|\n|          12|Shoulder clearly ...|1605.00|          61920.00|           12|       true|2024-12-02 00:04:...|cobertura|\n|          13|Pattern free buil...|1295.00|          51290.00|           13|       true|2024-12-02 00:04:...|cobertura|\n|          14|Or art whether cl...| 619.00|          89901.00|           14|      false|2024-12-02 00:04:...|cobertura|\n|          15|Adult change majo...| 863.00|          62004.00|           15|       true|2024-12-02 00:04:...|cobertura|\n|          16|Cut time accordin...|5784.00|          37657.00|           16|      false|2024-12-02 00:04:...|cobertura|\n|          17|Contain head laug...|2738.00|          10524.00|           17|      false|2024-12-02 00:04:...|cobertura|\n|          18|West early site a...|4629.00|          34757.00|           18|       true|2024-12-02 00:04:...|cobertura|\n|          19|Job expect includ...|7974.00|          13375.00|           19|       true|2024-12-02 00:04:...|cobertura|\n|          20|When open full ya...|3896.00|          91704.00|           20|       true|2024-12-02 00:04:...|cobertura|\n+------------+--------------------+-------+------------------+-------------+-----------+--------------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LeituraDelta\").getOrCreate()\n",
    "storageAccountName = \"datalakea04fb344bd6a3620\"\n",
    "sas_token = \"-\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.bronze.{storageAccountName}.blob.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "table_name = \"cobertura\"\n",
    "bronze_path = f\"wasbs://bronze@{storageAccountName}.blob.core.windows.net/{table_name}\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
